{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4792ae",
   "metadata": {},
   "source": [
    "# Regression example: predict RPM from audio+encoder recordings\n",
    "\n",
    "This notebook demonstrates how to build a regression dataset from CSVs produced by `audio_and_encoder.record_snippet` (columns: `time_seconds`, `audio_signal`, `rpm`) and train a baseline RPM regressor using the `regression_orchestrator.py` helper.\n",
    "\n",
    "Objectives:\n",
    "- Build train/test feature matrices using the same rich feature extractor used by the pipeline.\n",
    "- Compute labels as the average RPM over each time window.\n",
    "- Train a simple baseline regressor with scaling and cross-validation.\n",
    "- Evaluate performance and visualize predictions vs ground truth.\n",
    "\n",
    "Run cells sequentially. If you edited `regression_orchestrator.py`, re-run the import cell to pick up changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0829991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03596ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically import the regression orchestrator (keeps the notebook flexible)\n",
    "# This search is robust to the notebook being moved; it finds the helper anywhere in the repo.\n",
    "candidates = list(Path.cwd().rglob('regression_orchestrator.py'))\n",
    "if not candidates:\n",
    "    raise FileNotFoundError('Could not locate regression_orchestrator.py in the repository. Ensure the file is present under Feature_extraction_pipeline.')\n",
    "reg_path = candidates[0].resolve()\n",
    "spec = importlib.util.spec_from_file_location('regression_orch', str(reg_path))\n",
    "regmod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(regmod)  # type: ignore[attr-defined]\n",
    "run_regression_on_dataset = getattr(regmod, 'run_regression_on_dataset')\n",
    "print(f'Imported regression_orchestrator OK from {reg_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c62551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression dataset from CSVs under sample_data (adjust path if needed)\n",
    "data_root = Path('regression_data')\n",
    "res = run_regression_on_dataset(data_root, segment_seconds=10.0, overlap=0.5, train_fraction=0.8, buffer_seconds=0.5, feature_level='standard')\n",
    "X_train = res['train']['X']\n",
    "y_train = np.asarray(res['train']['y'], dtype=float)\n",
    "X_test = res['test']['X']\n",
    "y_test = np.asarray(res['test']['y'], dtype=float)\n",
    "print('Train shape:', X_train.shape, 'Train samples:', y_train.shape[0])\n",
    "print('Test shape :', X_test.shape,  'Test samples :', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection of label distribution\n",
    "if y_train.size:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.histplot(y_train, bins=30, kde=True, ax=ax[0])\n",
    "    ax[0].set_title('Train RPM distribution')\n",
    "    if y_test.size:\n",
    "        sns.histplot(y_test, bins=30, kde=True, ax=ax[1], color='orange')\n",
    "        ax[1].set_title('Test RPM distribution')\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('No labels found in dataset. Ensure CSVs under sample_data contain rpm values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85847cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: scaling + RandomForestRegressor with 5-fold CV (if enough samples)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "if X_train.size == 0 or y_train.size == 0:\n",
    "    print('No training data. Skipping model training.')\n",
    "else:\n",
    "    model = make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    n_samples = y_train.shape[0]\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42) if n_samples >= 5 else None\n",
    "    if cv is not None:\n",
    "        scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "        print('CV MSE (mean):', float(-np.mean(scores)), 'std:', float(np.std(scores)))\n",
    "    else:\n",
    "        print('Not enough samples for 5-fold CV; train on full set instead')\n",
    "    # fit and evaluate on test set\n",
    "    model.fit(X_train, y_train)\n",
    "    if X_test.size and y_test.size:\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f'Test MSE: {mse:.4f}, R2: {r2:.4f}')\n",
    "    else:\n",
    "        print('No test set available for evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Predicted vs True and residuals (if test available)\n",
    "if 'model' in globals() and hasattr(model, 'predict') and X_test.size and y_test.size:\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Predicted vs True\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].scatter(y_test, y_pred, alpha=0.6)\n",
    "    axs[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    axs[0].set_xlabel('True RPM')\n",
    "    axs[0].set_ylabel('Predicted RPM')\n",
    "    axs[0].set_title('Predicted vs True')\n",
    "\n",
    "    # Residuals distribution\n",
    "    residuals = y_test - y_pred\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=axs[1])\n",
    "    axs[1].set_title('Residuals (y_true - y_pred)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Feature importances (if RandomForest is present in the pipeline)\n",
    "    try:\n",
    "        rf = None\n",
    "        if hasattr(model, 'named_steps'):\n",
    "            rf = model.named_steps.get('randomforestregressor')\n",
    "        if rf is None and hasattr(model, 'steps'):\n",
    "            # try to find a RandomForestRegressor instance among steps\n",
    "            for _, step_obj in model.steps:\n",
    "                if step_obj.__class__.__name__ == 'RandomForestRegressor':\n",
    "                    rf = step_obj\n",
    "                    break\n",
    "\n",
    "        if rf is not None:\n",
    "            importances = rf.feature_importances_\n",
    "            fnames = None\n",
    "            if isinstance(res, dict) and isinstance(res.get('train'), dict):\n",
    "                fnames = res['train'].get('feature_names')\n",
    "            if not fnames:\n",
    "                fnames = [f'feat{i}' for i in range(importances.size)]\n",
    "            idx = np.argsort(importances)[::-1][:20]\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.barh([fnames[i] for i in idx[::-1]], importances[idx[::-1]])\n",
    "            ax.set_title('Top feature importances (RandomForest)')\n",
    "            plt.tight_layout()\n",
    "        else:\n",
    "            print('RandomForestRegressor not found in pipeline; skipping feature importances.')\n",
    "    except Exception as e:\n",
    "        print('Could not compute feature importances:', e)\n",
    "else:\n",
    "    print('Model not found or no test data available for visualization.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa226d",
   "metadata": {},
   "source": [
    "## Teaching notes and extensions\n",
    "- This example trains a basic RandomForest regressor. For curriculum, you can show the effect of different feature levels (`basic`/`standard`/`advanced`) by re-running the dataset build cell with `feature_level` changed.\n",
    "- To add RPM as an input feature instead of target only, compute the average RPM per segment and append as a column in `X` before training.\n",
    "- Try regression metrics beyond MSE: MAE, RMSE, or quantile regression for robust evaluation.\n",
    "- For time-series aware models, preserve temporal order and use rolling/sequence models (LSTM/TemporalConv).\n",
    "\n",
    "If you'd like, I can add a second notebook cell that shows hyperparameter tuning with `RandomizedSearchCV` or a small Keras/TensorFlow example for a neural baseline."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
